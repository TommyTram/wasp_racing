\documentclass{article}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{titlesec}
\usepackage{subfig}
\usepackage{todonotes}

\begin{document}

\title{WASP RL racing}
\author{
	First Author\\
	Department\\
	school\\
	email@edu
	\and
	Second Author\\
	Department\\
	school\\
	email@edu
	\and
	Third Author\\
	Department\\
	school\\
	email@edu
	\and
	Fourth Author\\
	Department\\
	school\\
	email@edu
}

\maketitle
\thispagestyle{empty}
\includegraphics[width=12cm]{img/torcs.png} % also works with logo.pdf
\clearpage
\tableofcontents
\clearpage
\section{Introduction}
\setcounter{page}{1}

\section{Background}

\subsection{Markov Decision Process}

\subsection{Reinforcement Learning}

\subsection{Hierarchical Reinforcement Learning}

Hierarchical Reinforcement Learning (HRL) differs from traditional RL in a way that the model that is being learned is a semi-MDP instead of a MDP. Here we not only learn a policy $\pi_{a,g}(s) \rightarrow a$ but also a policy $\pi_g(s) \rightarrow g$, that gives us a \textit{goal}. Initially we will assume that the $\pi_{a,g}$ policy is infact a PID-controller and what we actually want to learn is the $\pi_{g}$ policy.

\subsubsection{Goals}

A goal is simply a higher level action, such as \textit{overtake}, \textit{follow}, \textit{merge} in the AD context. 



\section{Setting}
We are dealing with two notably different settings. In one we learn a policy $\pi$ that maps states $s \in \mathcal{S}$ to actions $a \in \mathcal{A}$. That is, we are looking for a (stochastic/deterministic?) function $\{\pi,\mu\}(s) \rightarrow a$, or, in other words we want $p(a|s)$.

In the standard RL setting our action-space is merely the steering angle $\theta \in [-\pi,\pi]$, the acceleration $x_a \in [0,1]$ and the deceleration $x_d \in [0,1]$.

\subsection{Termination conditions}

We also have to take in mind the conditions that lead to the end of an episode. At the end of the episode we restart our experiment. If we do not have well-defined termination conditions we might get stuck in simulation.

The current termination conditions we use are, if car takes damage (hits a wall), if enough time steps has elapsed (to avoid getting stuck).

\subsection{HRL setting}

\subsubsection{Reward function}

We can think of two kinds of rewards, one that is intristic and shown every timestep. That is,  
\[
r_t = V_x \cos(\theta) - V_x\sin(\theta)-V_x|trackPos|
\]
scaled to approximately $[-1,1]$ for stability

But we also have another reward that we denote as the $\textit{terminal}$ reward,

\[
R_t = -T
\]

As of now in the DQN-algorithm we are using the intristic reward $r_t$ which also means we can learn not only outside of the episode but also inside of the episode.

\subsubsection{Decision maker}
We assume a $\epsilon-$greedy decision maker, with random exploration parameter $\epsilon_t$. We tune this $\epsilon_t$ parameter adaptively, starting at $\epsilon_0 = 1.0$ and asymptotically approaching $\epsilon_\infty = 0.01$. This to force the agent to occasionally explore even for long horizons.

\[
	\epsilon_0 + (\epsilon_\infty - \epsilon_0) e^{-\lambda t}
\]


\subsubsection{Action-space}
Our action-space $\mathcal{A}$ initially consists of three actions, namely, $\textit{left}, \textit{center}, \textit{right}$. These actions corresepond to the position in the track that we want to follow. 

\subsubsection{State-space}
The state-space $\mathcal{S}$ consists of a $29\times1$ vector of features such as, current track position, angle to the track, sensors of opponents and so on.

\subsubsection{Controller dynamics}

\paragraph{Velocity controller}

$v_{set}$ is our velocity setpoint. $x_a$ is our acceleration, $p_a$ is our P-parameter for the acceleration and $x_{v_x}$ is the current x-velocity

\[
	x_a = p_a (v_{set} - x_{v_x})
\]

If $x_a$ is negative, this gain is applied towards the brakes instead. Otherwise it is applied to the throttle.

\paragraph{Driving controller}

We have an offset denoted by $\delta$ that specifies the offset for the controller depending on the action selected. For ${\text{\{left,right,center}\}}$ these offsets are $\{-0.8, 0.0, 0.8\}$

\subsubsection{Training}

We use experience replay and (hopefully) batch normalization. We update the weights after each observation $(s,a) \rightarrow (r,s',a')$ and store it in our memory buffer. At every training step we sample a subset of this buffer and use it for training.

\section{Results}

\section{Conclusion}

\end{document}