\documentclass{article}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{titlesec}
\usepackage{subfig}
\usepackage{todonotes}

\begin{document}

\title{WASP Reinforcement Learning project course \\
	Final report}
\author{
	Ivo Batkovic\\
	Zenuity\\
	Chalmers\\
	ivo.batkovic@zenuity.com
	\and
	Hannes Eriksson\\
	Zenuity\\
	Chalmers\\
	hannese@chalmers.se
	\and
	Carl-Johan Hoel\\
	Volvo\\
	Chalmers\\
	carl-johan.hoel@volvo.com
	\and
	Tommy Tram\\
	Zenuity\\
	Chalmers\\
	tommy.tram@zenuity.com
}

\maketitle
\thispagestyle{empty}
\includegraphics[width=12cm]{img/torcs.png} % also works with logo.pdf
\clearpage
\tableofcontents
\clearpage
\section{Introduction}
\setcounter{page}{1}

\section{Background}

\subsection{Markov Decision Process}

\subsection{Reinforcement Learning}

\subsection{Hierarchical Reinforcement Learning}

Hierarchical Reinforcement Learning (HRL) differs from traditional RL in a way that the model that is being learned is a semi-MDP instead of a MDP. Here we not only learn a policy $\pi_{a,g}(s) \rightarrow a$ but also a policy $\pi_g(s) \rightarrow g$, that gives us a \textit{goal}. Initially we will assume that the $\pi_{a,g}$ policy is infact a PID-controller and what we actually want to learn is the $\pi_{g}$ policy.

\subsubsection{Goals}

A goal is simply a higher level action, such as \textit{overtake}, \textit{follow}, \textit{merge} in the AD context. 



\section{Setting}
We are dealing with two notably different settings. In one we learn a policy $\pi$ that maps states $s \in \mathcal{S}$ to actions $a \in \mathcal{A}$. That is, we are looking for a (stochastic/deterministic?) function $\{\pi,\mu\}(s) \rightarrow a$, or, in other words we want $p(a|s)$.

In the standard RL setting our action-space is merely the steering angle $\theta \in [-\pi,\pi]$, the acceleration $x_a \in [0,1]$ and the deceleration $x_d \in [0,1]$.

\subsection{Termination conditions}

We also have to take in mind the conditions that lead to the end of an episode. At the end of the episode we restart our experiment. If we do not have well-defined termination conditions we might get stuck in simulation.

The current termination conditions we use are, if car takes damage (hits a wall), if enough time steps has elapsed (to avoid getting stuck).

\subsection{HRL setting}

\subsubsection{Reward function}

We can think of two kinds of rewards, one that is intristic and shown every timestep. That is,  
\[
r_t = V_x \cos(\theta) - V_x\sin(\theta)-V_x|trackPos|
\]
scaled to approximately $[-1,1]$ for stability

But we also have another reward that we denote as the $\textit{terminal}$ reward,

\[
R_t = -T
\]

We can also define a third reward function that measures \textit{progress}

TODO write about it here

As of now in the DQN-algorithm we are using the intristic reward $r_t$ which also means we can learn not only outside of the episode but also inside of the episode.

\subsubsection{Decision maker}
We assume a $\epsilon-$greedy decision maker, with random exploration parameter $\epsilon_t$. We tune this $\epsilon_t$ parameter adaptively, starting at $\epsilon_0 = 1.0$ and asymptotically approaching $\epsilon_\infty = 0.01$. This to force the agent to occasionally explore even for long horizons.

\[
	\epsilon_0 + (\epsilon_\infty - \epsilon_0) e^{-\lambda t}
\]


\subsubsection{Action-space}
Our action-space $\mathcal{A}$ initially consists of three actions, namely, $\textit{left}, \textit{center}, \textit{right}$. These actions corresepond to the position in the track that we want to follow. 

\subsubsection{State-space}
The state-space $\mathcal{S}$ consists of a $29\times1$ vector of features such as, current track position, angle to the track, sensors of opponents and so on.

\subsubsection{Controller dynamics}

\paragraph{Velocity controller}

$v_{set}$ is our velocity setpoint. $x_a$ is our acceleration, $p_a$ is our P-parameter for the acceleration and $x_{v_x}$ is the current x-velocity

\[
	x_a = p_a (v_{set} - x_{v_x})
\]

If $x_a$ is negative, this gain is applied towards the brakes instead. Otherwise it is applied to the throttle.

\paragraph{Driving controller}

We have an offset denoted by $\delta$ that specifies the offset for the controller depending on the action selected. For ${\text{\{left,right,center}\}}$ these offsets are $\{-0.8, 0.0, 0.8\}$

\subsubsection{Actor-Critic}


\subsubsection{Training}

We use experience replay and (hopefully) batch normalization. We update the weights after each observation $(s,a) \rightarrow (r,s',a')$ and store it in our memory buffer. At every training step we sample a subset of this buffer and use it for training.

\section{Results}

\section{Extensions}

\subsection{Continuous actions}

If we consider a continuous lateral position offset instead of a discretized one we can learn how to position ourselves arbitrarily on the track. The step from a discrete representation to a continuous one would require a more suitable model. How to handle continuous actions in a DQN is not immediately trivial.  Consider the current mapping, $\{a_1,a_2,a_3\} = \{-0.8, 0.0, 0.8\}$, we could instead say that our actions $\mathcal{A} \in [-0.8,0.8]$. Then we could have a linear mapping from action $a \rightarrow \text{offset}$. The easiest way to do this would be to learn a policy network $\pi_\theta(a|s) = \mathbb{P}(a | s, \theta)$ to which we could apply a Gaussian policy.

\subsection{Planning}

This would require us to learn a transition model  $\mathbb{P}(s_{t+1}=s'|s_t=s,a_t=a)$. Perhaps one could consider a trivial model for the vehicle dynamics, such as the kinetic model of the car. The next state would then be easily calculated given the current position and the current velocity. However, for more complicated features such as the range finders, and the radial sensors this is not so trivial. So to do this well one would most likely have to learn the transition model rather than use the kinetic model as an approximation. A neural network could be used to learn this representation, having input neurons be $(s_t,a_t)$ and output neurons be $s_{t+1}$.

\subsection{Reward shaping}

Reward shaping would be interesting because we could then find a better reward function to use that would more closely match our true goal, minimizing the time it takes for the car to run a lap. Since we use an intristic reward that is received at every time step it is not certain that this reward function and the terminal reward function would lead to the same policy $\pi$, in fact it is very unlikely. Using reward shaping we could instead try to find a better reward function.

\subsection{MPC}

We could think of an extension of the current controller that instead a MPC.

\subsection{Actor-Critic}
To be able to manage a continuous action-space we move from a DQN-network that selects discrete actions to an actor-critic architecture. The actor network is $\pi_\theta$ and is a mapping from states $s$ to a distribution over actions $a \in \mathcal{A}$. So $a \sim \pi_\theta(a|s)$. $\theta$ are the parameters of our network. We also have a critic/value network $V_\phi^\pi$ that is a mapping from states $s$ and policy $\pi$ to a value $v \in \mathbb{R}$. The parameters of our critic network are $\phi$.
We can then update our prediction $\hat{V}_\phi^\pi (s)= r + \gamma \hat{V}_\phi^\pi (s')$. We also calculate the advantage function $\hat{A}^\pi (s,a) = r(s,a) + \gamma \hat{V}_\phi^\pi (s') - \hat{V}_\phi^\pi (s).$
We can then update our parameters $\theta$ according to the following;
\[
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta (a|s) \hat{A}^\pi (s,a)
\]

\section{Conclusion}

\section{What did we learn?}

\end{document}