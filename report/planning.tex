\documentclass{article}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{titlesec}
\usepackage{subfig}
\usepackage{todonotes}
\usepackage{pifont}

\begin{document}

\title{WASP Reinforcement Learning project course \\
		Planning report}
\author{
	Ivo Batkovic\\
	Zenuity\\
	Chalmers\\
	ivo.batkovic@zenuity.com
	\and
	Hannes Eriksson\\
	Zenuity\\
	Chalmers\\
	hannese@chalmers.se
	\and
	Carl-Johan Hoel\\
	Volvo\\
	Chalmers\\
	carl-johan.hoel@volvo.com
	\and
	Tommy Tram\\
	Zenuity\\
	Chalmers\\
	tommy.tram@zenuity.com
}

\maketitle
\thispagestyle{empty}
\clearpage
\section{Time plan}

In the beginning it took some time for us students to get organized. Not only that but we had some difficulty in getting the environment up and running. We decided that because of these implications the original plan would be too demanding, and we chose to scale it down a little. The things that were in the original plan that we now had to skip are; joining and competing in the TORCS Endurance competition. We believe that this would take additional time to set up and not only that, we might have been forced to alter our existing framework just for the sake of competing, which would lead to further disorganization. The other thing we talked about initially that we had to scale down was the cooperation with the WARA-CAT team and using a live vehicle to evaluate our final algorithms/policies. That would certainly be too demanding for a project of this size and it would be another framework we would have to learn.
Ideally what we come up with could easily be integrated into their software at a later point if necessary, but it is outside the scope of this course.

We also had a number of students drop out and/or not show interest in the course. It is then very hard for the remainder of us to plan and split up the work if we do not know who will contribute. 

Overall we think that the project schedule is delayed by about 3-4 weeks but we still believe we can deliver a report, some results and most of all, we have learned something.
\begin{table}
	\begin{center}
	\begin{tabular}{|l|l|c|}
	\hline
	Date & Type & Status \\
	20 September & Initial Meeting & \ding{51} \\
	& Background Reading & \ding{51} \\
	03 October & Trial robot entry & \ding{55} \\
	& Literature search & \ding{51} \\
	11 October & Draft plan and organisation & \ding{55} \\
	18 October & Final plan & \ding{55} \\
	11 November & First competition AI & \ding{55} \\
	05 December & Second competition AI & \\
	07 December & Draft report & \\
	11 December & Final results & \\
	15 December & Final report & \\
	\hline
	\end{tabular}
\caption{Original time plan}
\end{center}
\end{table}

\begin{table}
	\begin{center}
		\begin{tabular}{|l|l|c|}
			\hline
			Date & Type & Status \\
			20 September & Initial Meeting & \ding{51} \\
			& Background Reading & \ding{51} \\
			& Literature search & \ding{51} \\
			11 October & Draft plan and organisation & \ding{55} \\
			24 November & Final plan & \\
			07 December & Draft report & \\
			11 December & Final results & \\
			15 December & Final report & \\
			\hline
		\end{tabular}
		\caption{Our time plan proposal}
	\end{center}
\end{table}


We propose the very same time plan as originally discussed, but without the competition. Perhaps the report deadlines have to be moved slightly.

\section{Work plan}

In this project, we will create a robot/agent in an open racing car simulation, TORCS. The team is to enter the TORCS Racing Board (TRB) and compete with other teams. The championship consists of races over approximately 500km on various suitable road tracks. The robots are rewarded with points for their rank in the races. Based on these results the standings of the driver and team championships are computed. There will be approximately one race every 3-4 weeks.

\subsection{Scope}
Use reinforcement learning to optimize the lap time on a specific course in torcs, with no other vehicles present. Two approaches will be compared, using DQN and DDPG. We will implement the DQN algorithm, whereas the DDPG algorithm is already implemented.

\subsection{Problem formulation}
From the TORCS simulation use the sensor inputs and game state as a feature vector to feed to a DQN. 

The DQN will use a set of discrete actions corresponding to a predefined path (eg., keep to the left of the road center, keep to the middle, or right). From this we use a standard PID control to decide  the steering angle (either momentaneous or a look-ahead offset).

\subsection{Completed}

\begin{itemize}
	\item Defined the problem we want to solve
	\item Getting familiar with the TORCS framework
	\item Working with the existing DDPG solution
	\item Setting up our own DQN model
	\item Implementing a controller
\end{itemize}

\subsection{Pending}

\begin{itemize}
	\item Setting up the loading/saving for the DQN
	\item Adding more agents
	\item Collecting results
	\item Finishing the report
\end{itemize}

\subsection{Extensions}

\begin{itemize}
	\item Continuous actions
	\item Trajectory planning
	\item MPC-controller
	\item Reward shaping
	\item Joining the competition
	
\end{itemize}

\end{document}